{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOKb34XTTBWEajgYRtiqz9S",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RaviKcse08/datascience_projs/blob/main/RNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**RNN model for Natural Language Processing (NLP) text classification:**\n",
        "\n",
        "Natural Language Processing (NLP) is a sub-field of computer science and artificial intelligence, dealing with processing and generating natural language data. Although there is still research that is outside of machine learning, most NLP is now based on language models produced by machine learning.\n"
      ],
      "metadata": {
        "id": "m70_3v_L3dPq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.datasets import imdb\n",
        "from keras.preprocessing import sequence\n",
        "import keras\n",
        "import tensorflow as tf\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "VOCAB_SIZE = 88584\n",
        "\n",
        "(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words = VOCAB_SIZE)"
      ],
      "metadata": {
        "id": "aEqfK9WMwkFW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "RNNs are different from regular feed-forward neural networks as they process information differently. In the normal feed-forward, the information is processed following the layers. However,  RNN is using a loop cycle on the information input as consideration.\n",
        "\n",
        "\n",
        "Well we can work with IMDB movie review dataset from keras. This dataset contains 25,000 reviews from IMDB where each one is already preprocessed and has a label as either positive or negative.\n"
      ],
      "metadata": {
        "id": "3OY81bgI3ngA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"train_data shape\", train_data.shape)\n",
        "print(\"train_labels shape\", train_labels.shape)\n",
        "print(\"test_data shape\", test_data.shape)\n",
        "print(\"test_labels shape\", test_labels.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gNQdydg0xW1_",
        "outputId": "a135b5c3-e2f9-431f-ae76-81238fcf4646"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train_data shape (25000,)\n",
            "train_labels shape (25000,)\n",
            "test_data shape (25000,)\n",
            "test_labels shape (25000,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(train_data[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8L38e6Q7zRXF",
        "outputId": "91f1f1d5-bac6-4500-8e43-499e71ce782b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "189"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IzrlzTuzvQru"
      },
      "outputs": [],
      "source": [
        "from keras.datasets import imdb\n",
        "from keras.preprocessing import sequence\n",
        "import keras\n",
        "import tensorflow as tf\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "VOCAB_SIZE = 88584\n",
        "\n",
        "MAXLEN = 250\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "#(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words = VOCAB_SIZE)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_data=sequence.pad_sequences(train_data,MAXLEN)\n",
        "test_data=sequence.pad_sequences(test_data,MAXLEN)"
      ],
      "metadata": {
        "id": "aftq7GIPvgM3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "In the model, we are using a word embedding layer as the first layer and adding a LSTM layer afterwards that feeds into a dense node to get our predicted sentiment. The output dimension of the vectors generated by the embedding layer is 32.\n"
      ],
      "metadata": {
        "id": "3evyAjWe3rg0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model=tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(VOCAB_SIZE,32),\n",
        "    tf.keras.layers.LSTM(32),\n",
        "    tf.keras.layers.Dense(1,activation='sigmoid')\n",
        "])"
      ],
      "metadata": {
        "id": "1Oe0yFiovbsd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2pwKtpmkvlNZ",
        "outputId": "02818bc8-fd21-4bb9-8149-d0ea87d15df0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, None, 32)          2834688   \n",
            "                                                                 \n",
            " lstm (LSTM)                 (None, 32)                8320      \n",
            "                                                                 \n",
            " dense (Dense)               (None, 1)                 33        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2843041 (10.85 MB)\n",
            "Trainable params: 2843041 (10.85 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss=\"binary_crossentropy\",optimizer=\"rmsprop\",metrics=['accuracy'])\n",
        "history=model.fit(train_data,train_labels,epochs=10,validation_split=0.2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XrxU1Hisvqqm",
        "outputId": "6790f804-d58a-488c-faa5-00e441f67bd2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "625/625 [==============================] - 73s 114ms/step - loss: 0.4674 - accuracy: 0.7685 - val_loss: 0.3303 - val_accuracy: 0.8616\n",
            "Epoch 2/10\n",
            "625/625 [==============================] - 69s 111ms/step - loss: 0.2681 - accuracy: 0.8938 - val_loss: 0.3024 - val_accuracy: 0.8760\n",
            "Epoch 3/10\n",
            "625/625 [==============================] - 68s 108ms/step - loss: 0.2040 - accuracy: 0.9248 - val_loss: 0.3356 - val_accuracy: 0.8738\n",
            "Epoch 4/10\n",
            "625/625 [==============================] - 70s 112ms/step - loss: 0.1643 - accuracy: 0.9419 - val_loss: 0.3235 - val_accuracy: 0.8838\n",
            "Epoch 5/10\n",
            "625/625 [==============================] - 68s 109ms/step - loss: 0.1359 - accuracy: 0.9541 - val_loss: 0.4138 - val_accuracy: 0.8774\n",
            "Epoch 6/10\n",
            "625/625 [==============================] - 68s 108ms/step - loss: 0.1149 - accuracy: 0.9613 - val_loss: 0.3532 - val_accuracy: 0.8808\n",
            "Epoch 7/10\n",
            "625/625 [==============================] - 69s 110ms/step - loss: 0.0948 - accuracy: 0.9692 - val_loss: 0.3439 - val_accuracy: 0.8726\n",
            "Epoch 8/10\n",
            "625/625 [==============================] - 70s 112ms/step - loss: 0.0829 - accuracy: 0.9738 - val_loss: 0.3426 - val_accuracy: 0.8728\n",
            "Epoch 9/10\n",
            "625/625 [==============================] - 67s 108ms/step - loss: 0.0696 - accuracy: 0.9786 - val_loss: 0.4199 - val_accuracy: 0.8740\n",
            "Epoch 10/10\n",
            "625/625 [==============================] - 67s 108ms/step - loss: 0.0605 - accuracy: 0.9809 - val_loss: 0.3846 - val_accuracy: 0.8704\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " RNN model is able to achieve 87%+ accuracy"
      ],
      "metadata": {
        "id": "TI_6hepG33bJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 64\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "#train_data = train_data.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)"
      ],
      "metadata": {
        "id": "YzzRJFUo2nom"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word_index=imdb.get_word_index()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NrO6q8bG3MVs",
        "outputId": "7afb26d8-9667-4494-f698-f78193f6ccf9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb_word_index.json\n",
            "1641221/1641221 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def encode_text(text):\n",
        "    tokens=keras.preprocessing.text.text_to_word_sequence(text)\n",
        "    tokens=[word_index[word] if word in word_index else 0 for word in tokens]\n",
        "    return sequence.pad_sequences([tokens],MAXLEN)[0]"
      ],
      "metadata": {
        "id": "z7UqcaJN3JRS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(text):\n",
        "    encoded_text=encode_text(text)\n",
        "    pred=encoded_text.reshape(1,250)\n",
        "    result=model.predict(pred)\n",
        "    print(result[0])"
      ],
      "metadata": {
        "id": "xuFKFTHH3CqJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "positive_review=\"That was a good movie\"\n",
        "predict(positive_review)\n",
        "\n",
        "negative_review=\"this movie, so disappointing\"\n",
        "predict(negative_review)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y7wWuKUi3TgO",
        "outputId": "0965e21b-595c-4d71-ceb1-99cbae08654f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 38ms/step\n",
            "[0.4250168]\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "[0.38649306]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 180
        },
        "id": "vmK7hfqF3mx1",
        "outputId": "19275e66-09ed-4b8d-d0ae-485ed566d3e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'encoded' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-4989e6c2a098>\u001b[0m in \u001b[0;36m<cell line: 12>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecode_integers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoded\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'encoded' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model=tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(VOCAB_SIZE,32),\n",
        "    tf.keras.layers.LSTM(32),\n",
        "    tf.keras.layers.Dense(1,activation='sigmoid')\n",
        "])"
      ],
      "metadata": {
        "id": "Pu6AnBo33004"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#train_data = np.array(train_data)\n",
        "#train_labels = np.array(train_labels)"
      ],
      "metadata": {
        "id": "rw95bqKw4Op9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss=\"binary_crossentropy\",optimizer=\"rmsprop\",metrics=['accuracy'])\n",
        "history=model.fit(train_data,train_labels,epochs=2,validation_split=0.2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UbtUbCuB39yH",
        "outputId": "8851fbde-2426-4390-bb4d-06653bc069e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/2\n",
            "625/625 [==============================] - 71s 110ms/step - loss: 0.4558 - accuracy: 0.7765 - val_loss: 0.3157 - val_accuracy: 0.8660\n",
            "Epoch 2/2\n",
            "625/625 [==============================] - 72s 115ms/step - loss: 0.2611 - accuracy: 0.8978 - val_loss: 0.2881 - val_accuracy: 0.8878\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#try with different optimizer and loss function\n",
        "model.compile(loss=\"mse\",optimizer=\"adam\",metrics=['accuracy'])\n",
        "history=model.fit(train_data,train_labels,epochs=2,validation_split=0.2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lu0K61s940Q5",
        "outputId": "4b5d9ed4-9055-43bb-fffa-2d19ec0d5e88"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/2\n",
            "625/625 [==============================] - 90s 140ms/step - loss: 0.0597 - accuracy: 0.9219 - val_loss: 0.0931 - val_accuracy: 0.8744\n",
            "Epoch 2/2\n",
            "625/625 [==============================] - 86s 137ms/step - loss: 0.0311 - accuracy: 0.9622 - val_loss: 0.1160 - val_accuracy: 0.8620\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Incremental learning:\n",
        "Tree-based algorithms mostly remain used in a batch learning scenario.\n",
        "Incremental learning is useful for fraud detection because it is less resource-intensive as models can be updated often on the last chunks of data instead of having to be fully trained on the whole dataset from scratch every time.\n",
        "Neural networks have the advantage of being incremental by nature since their training is iterative.\n"
      ],
      "metadata": {
        "id": "0j_xryW-3xVi"
      }
    }
  ]
}